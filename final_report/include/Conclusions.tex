% CREATED BY DAVID FRISK, 2016
\chapter{Conclusions} \label{ch6}

The main objective of this thesis project was to develop a syntax-based Concept Alignment system and put it to the test in the context of domain-specific MT. The approach we propose basically consists in analyzing texts with a dependency parser, UDPipe, and comparing the resulting trees. For the MT evaluation, such dependency trees are converted to Abstract Syntax Trees, from which the rules of a multilingual generative phrase-structure grammar are derived. The framework we make use of when it comes to this grammar, GF, makes having a grammar sufficient to perform simple MT experiments.\smallskip

The tangible fruit of this work is a Haskell library, as well as a number of executables offering an easy to use and configure interface to perform both variants of CA, extraction and propagation, and a variety of kinds of evaluations. \smallskip

To assess the quality of the alignments obtained with such system, the extraction and propagation components were first tested without taking the target application context into account. \smallskip

When it comes to CE, the core part of this work, comparison against a pre-existing basic implementation shows significant improvements in terms of both the quality and the quantity of the resulting alignments. 
With respects to traditional statistical approaches, one of the most useful consequences of our syntax-based approach is that it makes it possible to identify correspondences between multiword, even discontinuous expressions. 
Another important advantage over standard word alignment techniques is that, even though it can still benefit from large amounts of data, our system works consistently well even when run on extremely small corpora. This is confirmed by one of our experiments, where we restrict it so to only extract one-to-many and many-to-one alignments and compare it with \texttt{fast\_align}. 
Furthermore, the alignment criteria our software makes use of are easy to modify, add and remove, paving the way to a variety of experiments. \smallskip

CP, a less studied variant of CA, has been explored in two different scenarios. On the one hand, it can be useful to generate large multilingual lexica if applied to more-than-bilingual parallel corpora. On the other, it can also be used to identify shared terminology between two bilingual corpora having one language in common. The first experiments show promising results, but suffer from the scarce availability of sentence-segmented large bilingual corpora. \smallskip

Finally, the CA module was adapted to the task of MT. The results of the first, small-scale experiments conducted in this sense highlight the strengths of GF in terms of NLG but also indicate the need for a more aggressive alignment selection strategy, as the numerous correct alignments are sometimes overshadowed by alternative incorrect ones. 

\section*{Future work} \label{future}
These results, while encouraging, suggest that there is still much room for improvement and that work in this area can be carried on in many different directions even constaining ourselves to CA \textit{per se}, without focussing on its MT applications. 

\subsection*{Further integration with statistical alignment techniques} 
With regards to the CA component itself, it is important to remember that both the quantity and the quality of the alignments are a direct consequence of those of the dependency trees they are obtained from. Apart from working on dependency parsers directly, a possibility is to experiment more with hybrid alignment approaches, integrating the results of statistical methods (which only depend on the quantity and quality of raw data rather than on that of their analyses) with those of our rule-based system. One way to do that has already been implemented, but has not been made use of in the final MT experiments due to the small size of the corpora involved.

\subsection*{Aligning verb phrases}
Early in this work we noticed how only aligning subtrees is not enough to identify all the concepts that are present in a pair of sentences. As a consequence, significant effort has been made in also aligning subtree heads correctly. While this increases the number of concepts the program is able to find, it does not cover all cases. For instance, aligning complete verb phrases (i.e. verbs together with their arguments) could prove extremely useful. Generalizing the approach used for head alignment could be a way of addressing the issue and would make it possible to use CA to construct ``rich'' lexical entries for verbs, whose lack, as discussed in Chapter \ref{ch3}, currently makes it hard to handle some types of common translations divergences.

\subsection*{Iterative CA}
The current implementations of CE and CP try to apply a set of priority-sorted criteria one after another to each pair of sentences in the corpus, allowing for later ranking of the alignments obtained based on the criteria their members match. Another possibility is to go through the entire corpus multiple times, for instance using only the strictest criteria first and only falling back to the less reliable ones at later iterations, until no more alignments are found.
The same hypothetical iterative algorithm can have the size of the alignments, and not (or not only) the criteria they have to satisfy, as a parameter. 

\subsection*{Optimizing propagation for multilingual corpora}
CP was not the main focus of this work. As noted in Chapter \ref{ch4}, however, optimizing it for the simpler scenario in which it is applied to a more-than-2-lingual parallel text is straightforward and would improve both the quality of the results and, arguably, running time. Once this is done, generating multi-, as opposed to bi-, lingual domain-specific grammars will be more realistic, especially if \texttt{gf-ud}'s grammar generation functionality will also be further developed in parallel.

\subsection*{Generalizing CE to $n$ languages}
As an alternative to this optimization, CE could be generalized to an arbitrary number of languages. The expectation is for the resulting concepts to be of slightly larger size than those obtained with the current system, since comparing more trees simultaneously increases the possibility of encountering small divergences, this term being intended in its broad sense. This approach would probably help identifying longer idiomatic expression and could provide a better way to deal with multilingual corpora, even though, while intuitive, it does present some implementation challenges. 

\subsection*{Stricter and language pair-specific criteria} 
When it comes to using the concepts as actual translation units, getting rid of incorrect alignments is crucial. While doing so manually is still more feasible than extracting them manually, it would be desirable to devise better alignment selection policies or stricter alignment criteria. One idea would is to tune the alignment criteria for the language pair at hand, both by removing the noisier ones for semantically related languages and by adding language-pair specific criteria. The criteria-specific statistics reported in Section \ref{eval2} can serve as a starting point for work in this direction.