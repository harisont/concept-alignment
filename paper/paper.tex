% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\title{Syntax-Based Concept Alignment for Domain-Specific Machine Translation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% TODO: authors info
\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
% TODO: abstract
\end{abstract}

\section{Introduction}
Grammar-based translation pipelines such as those based on Grammatical Framework (GF) \cite{TODO:} have been successfully employed in domain-specific Machine Translation (MT). 
What makes these systems well suited to the task is the fact that, when we constrain ourselves to a specific domain, where precision is most often more important than coverage, they can provide strong guarantees in terms of grammatical correctness. 

Nevertheless, lexical exactness is, in this context, just as important as grammaticality. 
An important part of the design of the Controlled Natural Language (CNL) the grammar in such a system describes becomes, then, the creation of a translation lexicon. 
In many cases, this is done for the most part manually, resulting in a time consuming task requiring significant linguistic knowledge. 
When the grammar is designed based on a parallel corpus of example sentences, it is possible to automate part of this process by means of statistical word and phrase alignment techniques \cite{TODO: a lot}. 
None of them is, however, suitable for the common case in which only a limited number of example sentences is available.

In this paper, we propose an alternative approach to the automation of this task. 
While still being data-driven, our method is grammar-based and, as such, capable of extracting meaningful correspondences even from individual sentence pairs. 

A further advantage of performing syntactic analysis is that we do not have to choose \textit{a priori} whether to focus on the word or phrase level. 
Instead, we can simultaneously operate at different levels of abstraction, thus extracting both single- and multiword, even non-contiguous, correspondences. 

For this reason, we refer to the task our system attempts to automate as \textit{Concept Alignment} (CA). 

This paper is structured as follows. 
Section \ref{methodology} starts by giving an overview of our approach to CA, comparing it to related work, to then focus on our algorithm for extracting correspondences.
It is concluded with a description of our method for converting the alignments obtained in this way to a GF translation lexicon.
After that, Section \ref{evaluation} presents the results of our first evaluation of the system.
Finally, Section \ref{conclusions} consists of a discussion of such results and some ideas for future work. 

\section{Methodology} \label{methodology}
The objective of CA is to find semantical correspondences between parts of multilingual parallel texts. 
We call \textit{concepts} the abstract units of translation, composed of any number of words, identified through this process, and represent them as \textit{alignments}, i.e. tuples of equivalent concrete expressions in different languages.

The basic use case for CA, which we refer to specifically as \textit{Concept Extraction} (CE), is the generation of a translation lexicon from a parallel text. This can be directly compared to the numerous existing word and phrase alignment techniques. 

% potentially move to future work directly
An interesting and less studied variant of CA is \textit{Concept Propagation} (CP), useful for cases where a set of concepts is already known and the goal is to identify the expressions corresponding to each of them in a new language, potentially even working with a different text in the same domain.
While our system implements basic CP functionalities, however, in this paper we focus on CE and restrict its application to bilingual corpora. 

As stated in the Introduction, most existing extraction solutions are based on statistical approaches and are, as a consequence, unsuitable for small datasets. 
Grammar-based approaches, making use of parallel treebanks and collectively referred to as \textit{tree-to-tree alignment methods}, have also been proposed \cite{TODO:}, but have historically suffered from the inconsistencies between the various constituency grammar formalisms used to define grammars for the different languages and from the insufficient degree of robustness of existing parses.

This work is a new attempt in the same direction, enabled by two multilinguality-oriented grammar formalisms developed over the course of the last 25 years: Universal Dependencies (UD) \cite{TODO:} and Grammatical Framework (GF) \cite{TODO:}.

While both formalisms, the former a dependency and the latter a constituency grammar, independently solve the former issue, UD is especially appealing since dependency trees are an easier target for parsing. 
As a consequence, several robust dependency parsers, such as \cite{TODO: UDPipe} and \cite{TODO: Standford} are available.

UD parsing alone would then be sufficient to extract (or propagate) tree-to-tree alignments, but not to automate the generation of a ready-to-use, morphologically-aware translation lexicon. This is where GF comes into play: after correspondences are inferred from a parallel text, our proposed system is able to convert them to GF grammar rules, easy to embed in a domain-specific grammar but also making it possible to immediately evaluate the system by carrying out small-scale translation experiment using pre-existing grammatical constructions implemented in GF's Resource Grammar Library (RGL) \cite{TODO:}. 
This is made possible by \texttt{gf-ud}, a conversion tool described in \cite{TODO:} and \cite{TODO:}.

Concretely, the system we propose requires the following elements, whose reciprocal relations are shown in Figure \ref{overview}: \smallskip

\begin{itemize}
  \item a UD parser
  \item an alignment module based on dependency tree comparison
  \item a program, based on \texttt{gf-ud}, that converts the alignments into GF grammar rules and uses them to construct a translation lexicon
\end{itemize} \smallskip

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/overview.png}
  \caption[System overview]{System overview. Parts in grey are currently at the prototype stage and will not be further discussed in this paper.} \label{elems}
%  \label{overview}
\end{figure}

\subsection{Extracting concepts} 
% algorithm description (generic on criteria, with heads, iteratively)
% pseudocode

\subsubsection{Alignment criteria}
% label matching
% pos equivalence
% known divergence -> cite Dorr, few examples
% known alignment -> SMT integration

% how tracking them is useful (but maybe in evaluation?)

\subsubsection{Pattern matching}
% look for specific UD patterns

\subsection{Generating grammar rules}

\section{Evaluation} \label{evaluation}

\section{Conclusions} \label{conclusions}
% tangible output and were it is

% TODO: see why it does not compile
% Entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
%\bibliographystyle{acl_natbib}

\end{document}
